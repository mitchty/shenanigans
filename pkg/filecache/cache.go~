package main

import (
	"crypto/sha256"
	"errors"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"path"
	"syscall"

	"github.com/adrg/xdg"
	"github.com/google/uuid"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

const MOI = "shenanigans"

func cacheDir() string {
	return path.Join(xdg.CacheHome, MOI)
}

func tmpDir() string {
	return path.Join(cacheDir(), "tmp")
}

// Function we call if we receive an os signal to exit
func cleanupCache() error {
	err := os.RemoveAll(tmpDir())
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}
	return nil
}

// To save on having to download some things from the remote this
// function handles downloading to ~/.cache/shenanigans/sha25sum All
// files are given like nix in that their checksum = their name
// effectively to determine if they need to be downloaded again or
// not.
//
// https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.6/images/openSUSE-Leap-15.6.x86_64-NoCloud.qcow2
func cliInit(ctx *pulumi.Context) error {
	//return cacheFile("http://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-16.0.0-ContainerHost-kvm-and-xen-Snapshot20240512.qcow2", "d133e94121b12c0f1f714eaf48a6ca447813b46f61a64cf8dc5cb1040da52628")
	return cacheFile("https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.6/images/openSUSE-Leap-15.6.x86_64-NoCloud.qcow2", "d0d98b6d7c9d904d0c9146b56bd34d04df0c9a27719c8fdd698d5b5319063de9")
}

// Downloads file to .cache/shenanigans/uuid and then validates the
// sha256sums match what was provided and then moves the file to its
// sha256sum in .config/shenanigans for further usage.
func cacheFile(uri string, shasum string) error {
	dlUuid := uuid.New().String()

	destFile := path.Join(cacheDir(), "cache", shasum)

	e, error := os.Stat(destFile)
	// We only download if we don't find a file already in
	// ~/.cache/sha25sum, if we have it there we assume its ok and
	// hasn't bitrotted.
	if !errors.Is(error, os.ErrNotExist) {
		if e.Size() != 0 {
			return nil
		}

		return nil
	}

	err := os.MkdirAll(tmpDir(), os.ModePerm)
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}

	dlFile := path.Join(tmpDir(), dlUuid)

	out, err := os.Create(dlFile)
	defer out.Close()
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}

	fmt.Printf("dl: %s\n", uri)
	resp, err := http.Get(uri)
	defer resp.Body.Close()
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}

	_, err = io.Copy(out, resp.Body)
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}

	dlSum, err := sha256Sum(dlFile)
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return err
	}

	if dlSum == shasum {
		_, error := os.Stat(destFile)

		if errors.Is(error, os.ErrNotExist) {
			os.Rename(dlFile, destFile)
		}

		fmt.Printf("reusing already cached file from uri %s expected sha256sum: %s found: %s\n", uri, shasum, dlSum)
	} else {

		// Don't leave turds around in ~/.cache/shenanigans/tmp
		os.Remove(dlFile)
		fmt.Printf("file from uri %s expected sha256sum: %s found: %s\n", uri, shasum, dlSum)
		return err
	}
	return nil
}

// Derp function to read a file and get checksum
func sha256Sum(path string) (string, error) {
	f, err := os.Open(path)
	if err != nil {
		_ = fmt.Errorf(err.Error())
		return "", err
	}
	defer f.Close()

	h := sha256.New()
	if _, err := io.Copy(h, f); err != nil {
		_ = fmt.Errorf(err.Error())
		return "", err
	}

	return fmt.Sprintf("%x", h.Sum(nil)), nil
}

// TODOConvert the caching to a component, and once golang supports
// dynamic components use that to control lifecycle sanely.
//
// Future me just use localcommand resources to control deletion
// resources and hard link setup to the cached file.
//
// That should cover golang doing the downloading of things to
// ~/.cache/shenanigans and the hardlinking and removal setup as well,
// this way other resources can parent this component for cascading
// changes.

// "Caching" pulumi resource for files.
//
// My horse for real data types I hate abusing strings, golang+pulumi
// really lack ways to make invalid states not representable.
//
// TODOhave it snag sha256ums from shasum files too
// That way I can detect if the remote file has changed?
//
// Also need to figureout an airgap solution. Future mitch problem
// sorry past mitch is still jerk.
type CacheRemoteFile struct {
	Remote    pulumi.StringInput
	Sha256sum pulumi.StringInput
}

// func NewVpc(ctx *pulumi.Context, name string, args *VpcArgs) (*ec2.Vpc, error) {
// 	vpc, err := ec2.NewVpc(ctx, name, &ec2.VpcArgs{
// 		CidrBlock: args.CidrBlock,
// 	})
// 	if err != nil {
// 		return nil, err
// 	}

// 	return vpc, nil
// }

// TODOalso setup Components for vm setup+k8s server/client
//
// Probably like: Giant pool o vms = 1 component ->
// k8s prime admin node = 1 component ->
// par (rest of the admin nodes, worker nodes) = one component per lhs/rhs? ->
// whatever else depends on k8s after all thats ok

// Global variable used to decide what main() should do.
var (
	Main = "pulumi"
)

// Note: this binary is both run by pulumi, and built to be ran on the
// vms that are built themselves.
//
// Basically, pulumi gets the default main.go, and from pulumi we
// rebuild the binary and set a linker arg to change out a global that
// controls which main we get.
//
// This way we can build the same main.go file and then copy it to the
// remotes for each group task to do stuff.
//
// Why do it like this? Simple, since we know we've the go compiler
// already, lets use it to build a binary to do the work on the vm's.
//
// This means we don't have to deal with ansible, chef, shell,
// whatever. The logic to "do stuff" is the same as the logic to build
// the stuff that does stuff. This makes the end user requirements
// that need to be installed to be simply: pulumi and the go compiler.
//
// Each vm "group" should be an argument. And each group should have a
// subgroup on action. This makes the pulumi side for any provider simple:
// - Pulumi gets vm's up and running up to the point they can be ssh'd into
// - Then we scp the remote binary to each node
// - Pulumi then runs that binary to "get crap done" for each group
// - We can then setup unit tests for all the logic local and remote
//
// Example: Lets say we have a group named k8s-server, and we simply
// have a create and destroy action for what to do to create a server
// or destroy it.
//
// cmd k8s-server create -> maps to execute k8s-server-create() on the vm
// Any args past that just map to anything a prime (first) node needs
// or to what a non prime or non first node needs.
//
// Example there might be the first node of a k8s server might need to
// be 'special' in some way. The rest might need that node to do
// 'something' before they can get their job done. Deciding when to
// run that is the pulumi side of things. Doing it is the binary and
// the group/action on the vm.
func main() {
	var err error
	if Main == "pulumi" {
		err = pulumi_main()
	} else {
		err = remote_main()
	}

	if err != nil {
		_ = fmt.Errorf(err.Error())
	}
}

// Any other string for main.Main is not pulumi
// ezpzlemonsqueezy.
//
// Note: we build binaries with stripping on, e.g:
// -s = omit symbol table
// -w = omit DWARF symbol table
// This makes the binary for a simple printf go from 14MiB to 1KiBish
// GOOS=linux GOARCH=amd64 go build -o remote-arm64 -ldflags="-s -w -X 'main.Main=remote'"
// note: the intent here is that libvirt could run
// qemu as 64 bit arm on x86_64 and then build/copy
// the same binary remotely. Theoretically windows
// could be the target too but thats really far off in
// the future.
func remote_main() error {
	switch os.Args[1] {
	case "initcheck": // This just exists to validate the binary after scp
		fmt.Printf("ok\n")
		os.Exit(0)
	case "hostname":
		// This is just silly ignore it
		hostname, err := os.Hostname()
		if err != nil {
			fmt.Println(err)
			return err
		}
		fmt.Printf("%s\n", hostname)
	case "k8s": // k8s subcommand hard coded to rke2 for now
		switch os.Args[2] {
		default: // TODO--flavor=rke2|k3s|?  etc... for now rke2 airgap only and all defaults future mitch can figure out how the hell I do customization or not.
			// Args for k8s for now are --prime which means initial (prime) node
			// --join ip
			// --token string filled in by pulumi config to be the uuid of the instance for the prime node and the other nodes
			// TODOworker(s)
		}
	default:
		fmt.Printf("you should not be running this command generally\n")
	}
	return nil
}

func pulumi_main() error {
	// trap on exit to clean up ~/.cache/shenanigans/tmp
	sigs := make(chan os.Signal, 1)
	signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)
	go func() error {
		<-sigs
		return cleanupCache()
		//		os.Exit(0)
	}()
	pulumi.Run(func(ctx *pulumi.Context) error {
		// goOs := "linux"
		// goArch := "amd64"
		// goBuildCommand, err := local.NewCommand(ctx,
		// 	fmt.Sprintf("go-build-remote-%s-%s", goOs, goArch),
		// 	&local.CommandArgs{
		// 		Create: pulumi.Sprintf("env GOOS=linux GOARCH=amd64 go build -ldflags=\"-s -w -X 'main.Main=remote'\""),
		// 		Delete: pulumi.Sprintf("rm -f %s", publicKeyFileName),
		// 	},
		// 	pulumi.DependsOn([]pulumi.Resource{key}),
		// )
		// if err != nil {
		// 	_ = fmt.Errorf(err.Error())
		// 	return err
		// }

		stack := ctx.Stack()
		ctx.Export("stack", pulumi.String(stack))

		err := cliInit(ctx)
		if err != nil {
			return err
		}

		err = createVirtualMachines(ctx)
		if err != nil {
			return err
		}
		return nil
	})
	return nil
}
