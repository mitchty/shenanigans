package libvirt

import (
	"fmt"

	system "deploy/pkg/system"

	"github.com/pulumi/pulumi-command/sdk/go/command/remote"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config"
)

// What do these vm structures depend upon? Make sure thats all setup first
type Prereqs struct {
	vmRequiredResources []pulumi.Resource
	otherResources      []pulumi.Resource

	// Note this is largely copies of the required resources, but here for
	// specific use cases where the specific resource is needed for certain
	// purposes
	securityGroup *networking.SecGroup
	sshKey        *compute.Keypair
}

// For now have a global "just setup stuff I need first" function that callers
// call once.
//
// Can figure out a better way for multiple vm's to do stuff later.
func SetupPrereqs(ctx *pulumi.Context, nameSuffix string, sshResources *system.SshData) (Prereqs, error) {
	// Stuff that "must" be present
	var required []pulumi.Resource

	// Stuff that can wait, or isn't critical generally
	var other []pulumi.Resource

	openstackKeyName := fmt.Sprintf("key-%s", nameSuffix)
	openstackKey, err := compute.NewKeypair(ctx, openstackKeyName, &compute.KeypairArgs{
		Name:      pulumi.String(openstackKeyName),
		PublicKey: &sshResources.CaKey.PublicKeyOpenssh,
	})

	secGroupName := fmt.Sprintf("secgroup-%s", nameSuffix)
	secGroup, err := networking.NewSecGroup(ctx, secGroupName, &networking.SecGroupArgs{
		Description: pulumi.String(fmt.Sprintf("Auto generated security group setup for %s", secGroupName)),
	})
	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
	}
	secGroupRuleNamePrefix := fmt.Sprintf("secgrouprule-%s", nameSuffix)

	TcpIn, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "tcp-in"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("ingress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(1),
		PortRangeMax:    pulumi.Int(65535),
		Protocol:        pulumi.String("tcp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	required = append(required, TcpIn)

	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
	}

	UdpIn, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "udp-in"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("ingress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(1),
		PortRangeMax:    pulumi.Int(65535),
		Protocol:        pulumi.String("udp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
	}

	required = append(required, UdpIn)

	// any tcp out egress as thats also needed
	tcpOut, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "tcp-out"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("egress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(1),
		PortRangeMax:    pulumi.Int(65535),
		Protocol:        pulumi.String("tcp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	other = append(other, tcpOut)

	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup}, err
	}

	// and icmp ingress is needed for some sanity too
	icmpIn, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "icmp-in"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("ingress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(0),
		PortRangeMax:    pulumi.Int(0),
		Protocol:        pulumi.String("icmp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	other = append(other, icmpIn)

	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
	}

	// and icmp egress is needed for some sanity too
	icmpOut, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "icmp-out"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("egress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(0),
		PortRangeMax:    pulumi.Int(0),
		Protocol:        pulumi.String("icmp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	other = append(other, icmpOut)

	if err != nil {
		return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
	}

	// finally let any udp traffic out too
	udpOut, err := networking.NewSecGroupRule(ctx, fmt.Sprintf("%s-%s", secGroupRuleNamePrefix, "udp-out"), &networking.SecGroupRuleArgs{
		Direction:       pulumi.String("egress"),
		Ethertype:       pulumi.String("IPv4"),
		PortRangeMin:    pulumi.Int(0),
		PortRangeMax:    pulumi.Int(0),
		Protocol:        pulumi.String("udp"),
		RemoteIpPrefix:  pulumi.String("0.0.0.0/0"),
		SecurityGroupId: secGroup.ID(),
	})

	other = append(other, udpOut)

	return Prereqs{vmRequiredResources: required, otherResources: other, securityGroup: secGroup, sshKey: openstackKey}, err
}

type VMData struct {
	Required         []pulumi.Resource
	RemoteConnection *remote.ConnectionArgs
	InstanceResource *compute.Instance
}

func CreateVM(ctx *pulumi.Context, index int, nameSuffix string, sshResources *system.SshData, osData system.OSData, prereqs Prereqs) (VMData, error) {
	vmName := fmt.Sprintf("vm-%s", nameSuffix)

	cfg := config.New(ctx, "")

	// TODO: Hard coding in the knowledge of k8s servers vs workers, former get a smaller flavor future me make it configurable
	servers, err := cfg.TryInt("k8s/servers")
	if err != nil {
		servers = 0
	}

	defaultFlavor := cfg.Get("openstack/defaultflavor")

	// Use this as the default if not specified
	if defaultFlavor == "" {
		defaultFlavor = "standard.1"
	}

	flavor := defaultFlavor

	// Use this flavor instead of default in this case.
	if servers > 0 && index < servers {
		serverFlavor := cfg.Get("openstack/serverflavor")
		if serverFlavor == "" {
			serverFlavor = "standard.2"
		}
		flavor = serverFlavor
	}

	// TODO: Convert cloud-config to a function call probably and as its
	// yaml run it through some yaml lib instead of being lazy and sprintf
	// formatting strings like a heathen
	cloudConfig := fmt.Sprintf("#cloud-config\nfqdn: %s\nchpasswd:\n  list: |\n    - root:changeme\n    - %s:changeme\n  expire: False", vmName, osData.DefaultUser)
	ctx.Export("cloudConfig", pulumi.String(cloudConfig))

	instance, err := compute.NewInstance(ctx, vmName, &compute.InstanceArgs{
		FlavorName: pulumi.String(flavor),
		ImageName:  pulumi.String(osData.OSImageName),
		KeyPair:    prereqs.sshKey.Name,
		Networks: compute.InstanceNetworkArray{
			compute.InstanceNetworkArgs{
				Uuid: pulumi.String("fa9206d7-68c0-477e-aafb-d6e12317c4f0"),
			},
		},
		SecurityGroups: pulumi.StringArray{
			prereqs.securityGroup.Name,
		},
		UserData: pulumi.String(cloudConfig),
	})
	if err != nil {
		return VMData{InstanceResource: instance, Required: nil, RemoteConnection: nil}, nil
	}

	connectionArgs := remote.ConnectionArgs{
		Host:       instance.AccessIpV4,
		User:       pulumi.String(osData.DefaultUser),
		PrivateKey: sshResources.CaKey.PrivateKeyPem,
	}

	var nodeRequired []pulumi.Resource
	nodeOnline, err := remote.NewCommand(ctx, fmt.Sprintf("%s ssh?", vmName), &remote.CommandArgs{
		Connection: connectionArgs,
		Create:     pulumi.String("uptime"),
	}, pulumi.DependsOn([]pulumi.Resource{instance}),
	)
	if err != nil {
		return VMData{InstanceResource: instance, Required: nil, RemoteConnection: &connectionArgs}, nil
	}
	nodeRequired = append(nodeRequired, nodeOnline)

	return VMData{InstanceResource: instance, Required: nodeRequired, RemoteConnection: &connectionArgs}, err
}

// Simple wrapper for CreateVM but also sets up /etc/hosts once all vm's are online and the proxy data as well.
func CreateVMs(ctx *pulumi.Context, count int, nameSuffix string, sshResources *system.SshData, osData system.OSData, prereqs Prereqs) ([]VMData, error) {
	var vms []VMData
	var vmsRet []VMData
	var allDeps []pulumi.Resource

	// Setup an http proxy if the config has one specified before declaring
	// the node OK to upstream users.
	cfg := config.New(ctx, "")
	vmProxy := cfg.Get("vm/proxy")

	for idx := 0; idx < count; idx++ {
		vm, err := CreateVM(ctx, idx, fmt.Sprintf("%s-idx-%d", nameSuffix, idx), sshResources, osData, prereqs)
		if err != nil {
			return vms, err
		}
		vms = append(vms, vm)

		allDeps = append(allDeps, vm.Required...)
		scpVm, err := remote.NewCopyFile(ctx, fmt.Sprintf("copy vm setup script to vm%d", idx), &remote.CopyFileArgs{
			Connection: vm.RemoteConnection,
			LocalPath:  pulumi.String("vm.sh"),
			RemotePath: pulumi.String("/tmp/vm.sh"),
		}, pulumi.DependsOn(vm.Required))

		if err != nil {
			return vmsRet, err
		}

		vm.Required = append(vm.Required, scpVm)
		if vmProxy != "" {
			scpProxy, err := remote.NewCopyFile(ctx, fmt.Sprintf("copy proxy setup script to vm%d", idx), &remote.CopyFileArgs{
				Connection: vm.RemoteConnection,
				LocalPath:  pulumi.String("proxy.sh"),
				RemotePath: pulumi.String("/tmp/proxy.sh"),
			}, pulumi.DependsOn(vm.Required))

			if err != nil {
				return vmsRet, err
			}

			vm.Required = append(vm.Required, scpProxy)

			proxy, err := remote.NewCommand(ctx, fmt.Sprintf("idx%d: setup proxy", idx), &remote.CommandArgs{
				Connection: vm.RemoteConnection,
				// TODO: make this stuff vars as needed for non ubuntu/etc....
				// OLD
				// hackCidr = "172.29.242.0/21"
				Create: pulumi.Sprintf("sudo sh /tmp/proxy.sh %s /etc/profile.d/http-proxy.sh 10.103.16.0/22 | tee /var/tmp/proxy.log", vmProxy),
			}, pulumi.DependsOn(vm.Required))

			if err != nil {
				return vmsRet, err
			}
			vm.Required = append(vm.Required, proxy)
		}
		vmsRet = append(vmsRet, vm)

		runVm, err := remote.NewCommand(ctx, fmt.Sprintf("idx%d: vm.sh", idx), &remote.CommandArgs{
			Connection: vm.RemoteConnection,
			Create:     pulumi.Sprintf("sudo sh /tmp/vm.sh"),
		}, pulumi.DependsOn(vm.Required))

		if err != nil {
			return vmsRet, err
		}
		vm.Required = append(vm.Required, runVm)
	}

	// Huge hack, just append to /etc/hosts the host/ip combo, means a lot
	// of ssh as the node count increases but meh not worth the bother right
	// now.
	for idx, vm := range vms {
		for iidx, ivm := range vms {
			thisHostEntry := pulumi.Sprintf("printf \"%s\t$(echo %s | sed -e 's/.\\{8\\}$//')\n\" | sudo tee -a /etc/hosts", ivm.InstanceResource.AccessIpV4, ivm.InstanceResource.Name)

			hostentry, err := remote.NewCommand(ctx, fmt.Sprintf("idx%d: add idx%d to /etc/hosts", idx, iidx), &remote.CommandArgs{
				Connection: vm.RemoteConnection,
				Create:     thisHostEntry,
			}, pulumi.DependsOn(vm.Required))

			if err != nil {
				return vmsRet, err
			}
			ivm.Required = append(ivm.Required, hostentry)
		}
	}

	// for _, vm := range vms {
	// 	vms.Required = append(vms.Required, vm.Required[:]...)
	// }

	return vms, nil
}
